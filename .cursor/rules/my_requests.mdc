
---
description:
globs:
alwaysApply: true
---

From now on, I will write my request in the my_reuquests.mdc. Check my requests and answer or do them. ThenÙˆ delete completed requests from my_reuquests.mdc.

Use UV instead of pip in the dockerfile.

faiss indices are too large and it take too long to load all of them, and 30s timeout happens. What do you suggest to fix this problem? I think it would be good to store faiss indices on the RAM.

Delete vector_search from LLM agent available tools and everywhere else (specially mdc files).

Improve tools description so that the LLM can more intelligently select tools.

Run tests in .\run_test_scripts\run_scenarios_sample.py in paralel to boost speed. Add number of worker arg inside the script.

decrease size of extract_product_name to make it faster.

* set timeout not to continue with previous request.

*** 30s timeout for responsding too

add response_format={"type": "json_object"}, to llm calls

------------------------------
Scenario 1:
+ add a LLM agent call and give top result of bm25_search to find the correct product between them.

--------------------
Scenario 2:

<!-- Scenario 2:
In scenario2 tests (available in .\server_tests\scenario2.json) user has a question about a specific product. After finding the product using extract_product_name and bm25_search we should pass the product id to answer_question_about_a_product that first retrieves the product data (features, price, etc) from dataset. Then, pass the product data with the user request to the LLM agent and the LLM agent should answer the question about the product based on the user question and the product data. -->

---------------------
Scenario 3:

Consider english_names as well as persian names in product table.

Maybe I can do the export name task using the router LLM agent.

Use markdown in all prompts and put all of them between """prompt should be surrounded like this""".

Can I cache fixed parts of prompts to boost speed of LLM call?

-----
In scenarios 2 and 3 there shouldn't be any clarification questions and we should respond to the query immediately. I have deleted "# Ask for clarification to identify the exact product" part in scenarios 2 and 3 in main.py.

-----------------------------
Scenario 4:

<!-- Use tfidf for distinguishing products? -->

When second and ... requests come they should continue their flow in scenario 4 flow (they should skip scenario_number finding part).

In scenario 4 in first 3 clarifying questions find 50 most similar products using bm25_search (after finding name of the product using extract_product_name). Give the 50 most similar products to the LLM agent and ask to ask more clarifying questions. Name of the product should be updated based on user answers in before passing to bm25_search. We can create a new tool that updates product name based on answer's of the user to the clarifying questions or just pass all the user requests to the extract_product_name. The 4th question should be specifically related to the shop to find member_random_keys (we assume that before asking this question we found the base product). In the last turn we should tell the LLM the to select a product product and in the last response we should return the member_random_keys as mentioned in task_description.

pick_best_member_for_base shouldn't always return the member with the lowest price. Instead, it should ask about the price and the seller's attributes (e.g. has_warranty) and check all user constraints to find the shop that the user want to buy the product from. In the end, the output of our service should be member_random_keys. Check the description of scenario 4 as well in task_description.mdc.

Regarding scenario 4, it is also possible to check the shop_id with the user. This can be done in the last clarifying question if we already found the product.

<!-- In some requests the user wants a product, but they don't know the exact product. They want help to find the desired product with some constraints that they usually put in their request. The LLM agent should help the user find a product based on constraints by asking some clarifying questions. It would be good to search and gather some products that meet the initial constraints and then start to ask some clarifying questions based on features and price of those products. -->

<!-- Scenario 4 should enter a in a some kind of loop.  I think it is always better to return the response after the fifth user request, because we can gain more info about the requested product. Therefore, we can make sure that we have found the correct product. -->
-----------------------------------
Scenario 5:

*** Answer without generating sql query because of time limit.

comparison_extract_products should consider that if a product ID (6 lowercase letters) is given (next to its name) inside the user query, it should only return the product ID instead of its name. Then, only that product (the product whose ID is in the query) will skip bm25_search (becuase its id is already known).
In scenario 5, compare_two_products should also a method like _generate_sql_for_sellers to generate sql query based on the request. Then, this generated sql query should be run for each of the products in the comparison and the results of this sql query should be give to the LLM agent to generate the final response and determining the winner product.

<!-- comparison_extract_products should be modified in a way that it can extract two or more names from the query (now it extracts exactly two products, but there may be more than two products). In addition, 
Based on the fact that in scenario 5, there may be more than two products, compare_two_products should be modified to handle more than two products. -->